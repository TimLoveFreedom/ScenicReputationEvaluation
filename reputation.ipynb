{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\anaconda\\envs\\reputation\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba \n",
    "import multiprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.utils import shuffle \n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout,Activation\n",
    "from keras.models import model_from_yaml\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set parameters(word2vec)\n",
    "vocab_dim = 200\n",
    "maxlen = 100\n",
    "n_iterations = 1  # ideally more..\n",
    "n_exposures = 10\n",
    "window_size = 7\n",
    "cpu_count = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/train_first.csv')\n",
    "#data=data.drop(['Id'],axis=1)\n",
    "#data.head()\n",
    "data=data.sample(frac=1).reset_index(drop=True) \n",
    "\n",
    "#划分x  y\n",
    "X=data['Discuss']\n",
    "Y=data['Score']\n",
    " \n",
    "#y ONEHOT\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit([1,2,3,4,5])\n",
    "Y=lb.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我只能感叹以前人的建筑理念太超前，总是建造出比较宏伟值得人去纪念。小的时候真的有摸过回音壁，可是现在回音壁也只能看看摸不到的状态，很喜欢天坛的建筑，总觉得又梦回清朝，感受着这偌大的北京城。'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加载停用词\n",
    "def get_stopwords(path):\n",
    "    return [line.strip() for line in open(path,'r',encoding='utf-8').readlines()]\n",
    "#句子去停用词\n",
    "def removestopwords(sentence):\n",
    "        stopwords_list=get_stopwords('data/stopwords.txt')\n",
    "        outstr=[]\n",
    "        for word in sentence:\n",
    "            if not word in stopwords_list:\n",
    "                if word!='\\n' and word!='\\t':\n",
    "                     outstr.append(word)\n",
    "        return outstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\zhanggd\\AppData\\Local\\Temp\\5\\jieba.cache\n",
      "Loading model cost 2.688 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#分词 并去掉停用词\n",
    "def cut(sentence):\n",
    "    return removestopwords(jieba.cut(sentence))\n",
    "#分词后的word\n",
    "sentences=[cut(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['只能',\n",
       " '感叹',\n",
       " '建筑',\n",
       " '理念',\n",
       " '太超前',\n",
       " '建造',\n",
       " '宏伟',\n",
       " '值得',\n",
       " '人去',\n",
       " '纪念',\n",
       " '真的',\n",
       " '摸',\n",
       " '回音壁',\n",
       " '回音壁',\n",
       " '只能',\n",
       " '摸不到',\n",
       " '状态',\n",
       " '喜欢',\n",
       " '天坛',\n",
       " '建筑',\n",
       " '总',\n",
       " '梦回',\n",
       " '清朝',\n",
       " '感受',\n",
       " '偌大',\n",
       " '北京城']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "def get_vocab(sentences):\n",
    "    counts = Counter(list(itertools.chain.from_iterable(sentences)))\n",
    "    #选择超过10次的value\n",
    "    vocab_list=[]\n",
    "    for word in counts:\n",
    "        if counts[word]>=10:\n",
    "            vocab_list.append(word)        \n",
    "    vocab = sorted(vocab_list)\n",
    "    vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "    return vocab,vocab_to_int\n",
    "                    \n",
    "#获取字典  字典索引表\n",
    "vocab,vocab_to_int=get_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence2int(sentences,vocab_to_int):\n",
    "    reviews_ints = []\n",
    "    for each in sentences:\n",
    "        int_eachsententce=[]\n",
    "        for word in each:\n",
    "            if word in vocab_to_int:\n",
    "                int_eachsententce.append(vocab_to_int[word])\n",
    "            else:\n",
    "                int_eachsententce.append(0)\n",
    "        reviews_ints.append(int_eachsententce)\n",
    "    reviews_ints=sequence.pad_sequences(reviews_ints, maxlen=maxlen)\n",
    "    return reviews_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=get_sentence2int(sentences,vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#搭建网络结构\n",
    "input_x=tf.placeholder(shape=[None,None],dtype=tf.int32,name='input_x')\n",
    "label=tf.placeholder(shape=[None,None],dtype=tf.int32,name='target')\n",
    "lr=tf.placeholder(dtype=tf.float32,name='learning_rate')\n",
    "#sequence_lengths=tf.placeholder(shape=[None],dtype=tf.int32,name='sequence_lengths')\n",
    "keep_prob=tf.placeholder(dtype=tf.float32,name='keep_prob')\n",
    "#embedding_placeholder = tf.placeholder(tf.float32, [None, None],name='embedding_placeholder')\n",
    "#super params\n",
    "hidden_dim=128\n",
    "batch_size=100\n",
    "layer_num=2\n",
    "#bilstm\n",
    "# with tf.variable_scope('bi_lstm'):\n",
    "#     #lstm层\n",
    "#     lstm_fw_cell=rnn.BasicLSTMCell(hidden_dim,forget_bias=1.0,state_is_tuple=True)\n",
    "#     lstm_bw_cell=rnn.BasicLSTMCell(hidden_dim,forget_bias=1.0,state_is_tuple=True)\n",
    "#     #dropout\n",
    "#     lstm_fw_cell=rnn.DropoutWrapper(cell=lstm_fw_cell,input_keep_prob=1.0,out_keep_prob=keep_prob)\n",
    "#     lstm_bw_cell=rnn.DropoutWrapper(cell=lstm_bw_cell,input_keep_prob=1.0,out_keep_prob=keep_prob)\n",
    "#     #多层lstm\n",
    "#     cell_fw=rnn.MultiRNNCell([lstm_fw_cell]*layer_num, state_is_tuple=True)\n",
    "#     cell_bw=rnn.MultiRNNCell([lstm_fw_cell]*layer_num, state_is_tuple=True)\n",
    "#     #初始状态\n",
    "#     initial_state_fw=cell_fw.zero_state(batch_size,tf.float32)\n",
    "#     initial_state_bw=cell_bw.zero_state(batch_size,tf.float32)\n",
    "\n",
    "\n",
    "#with tf.variable_scope('embedding'):\n",
    "#embedding = tf.Variable(tf.constant(0.0, shape=[len(index_dict)+1,vocab_dim]),trainable=True, name=\"embedding\")\n",
    "embedding = tf.Variable(tf.random_uniform((len(vocab)+1,vocab_dim), -1, 1),name='embedding')\n",
    "#tf.assign(embedding,embedding_placeholder)\n",
    "#print(embedding)\n",
    "embed = tf.nn.embedding_lookup(embedding, input_x)\n",
    "#lstm\n",
    "#with tf.variable_scope('lstm'):\n",
    "\n",
    "def get_lstm_dropout():\n",
    "    lstm_cell=tf.contrib.rnn.BasicLSTMCell(hidden_dim)\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell=lstm_cell,output_keep_prob=keep_prob)\n",
    "cell=tf.contrib.rnn.MultiRNNCell([get_lstm_dropout() for _ in range(layer_num)] )\n",
    "initial_state=cell.zero_state(tf.shape(input_x)[0],tf.float32)\n",
    "outputs, final_state=tf.nn.dynamic_rnn(cell,embed,initial_state=initial_state)\n",
    "\n",
    "#with tf.variable_scope('optmizer'):\n",
    "predictions=tf.contrib.layers.fully_connected(outputs[:,-1],5,activation_fn=tf.nn.softmax)\n",
    "cost=tf.losses.mean_squared_error(label,predictions)\n",
    "optimizer=tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "#validation accuracy\n",
    "correct_pred=tf.equal(tf.cast(tf.round(predictions),tf.int32),label)\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#batching\n",
    "def get_batching(x,y,batch_size=100):\n",
    "    n_batches=len(x)//batch_size\n",
    "    x,y=x[:n_batches*batch_size],y[:n_batches*batch_size]\n",
    "    for ii in range(0,len(x),batch_size):\n",
    "        yield x[ii:ii+batch_size],y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10 Iteration: 50 Train loss: 0.106\n",
      "Epoch: 0/10 Iteration: 100 Train loss: 0.112\n",
      "Val acc: 0.841\n",
      "Epoch: 0/10 Iteration: 150 Train loss: 0.099\n",
      "Epoch: 0/10 Iteration: 200 Train loss: 0.104\n",
      "Val acc: 0.845\n",
      "Epoch: 0/10 Iteration: 250 Train loss: 0.089\n",
      "Epoch: 0/10 Iteration: 300 Train loss: 0.103\n",
      "Val acc: 0.848\n",
      "Epoch: 0/10 Iteration: 350 Train loss: 0.100\n",
      "Epoch: 0/10 Iteration: 400 Train loss: 0.102\n",
      "Val acc: 0.850\n",
      "Epoch: 0/10 Iteration: 450 Train loss: 0.111\n",
      "Epoch: 0/10 Iteration: 500 Train loss: 0.106\n",
      "Val acc: 0.852\n",
      "Epoch: 0/10 Iteration: 550 Train loss: 0.075\n",
      "Epoch: 0/10 Iteration: 600 Train loss: 0.101\n",
      "Val acc: 0.853\n",
      "Epoch: 0/10 Iteration: 650 Train loss: 0.089\n",
      "Epoch: 0/10 Iteration: 700 Train loss: 0.111\n",
      "Val acc: 0.854\n",
      "Epoch: 0/10 Iteration: 750 Train loss: 0.098\n",
      "Epoch: 0/10 Iteration: 800 Train loss: 0.097\n",
      "Val acc: 0.855\n",
      "Epoch: 1/10 Iteration: 850 Train loss: 0.097\n",
      "Epoch: 1/10 Iteration: 900 Train loss: 0.102\n",
      "Val acc: 0.856\n",
      "Epoch: 1/10 Iteration: 950 Train loss: 0.101\n",
      "Epoch: 1/10 Iteration: 1000 Train loss: 0.091\n",
      "Val acc: 0.855\n",
      "Epoch: 1/10 Iteration: 1050 Train loss: 0.082\n",
      "Epoch: 1/10 Iteration: 1100 Train loss: 0.097\n",
      "Val acc: 0.856\n",
      "Epoch: 1/10 Iteration: 1150 Train loss: 0.087\n",
      "Epoch: 1/10 Iteration: 1200 Train loss: 0.089\n",
      "Val acc: 0.856\n",
      "Epoch: 1/10 Iteration: 1250 Train loss: 0.099\n",
      "Epoch: 1/10 Iteration: 1300 Train loss: 0.102\n",
      "Val acc: 0.857\n",
      "Epoch: 1/10 Iteration: 1350 Train loss: 0.071\n",
      "Epoch: 1/10 Iteration: 1400 Train loss: 0.096\n",
      "Val acc: 0.856\n",
      "Epoch: 1/10 Iteration: 1450 Train loss: 0.084\n",
      "Epoch: 1/10 Iteration: 1500 Train loss: 0.110\n",
      "Val acc: 0.856\n",
      "Epoch: 1/10 Iteration: 1550 Train loss: 0.094\n",
      "Epoch: 1/10 Iteration: 1600 Train loss: 0.084\n",
      "Val acc: 0.856\n",
      "Epoch: 2/10 Iteration: 1650 Train loss: 0.089\n",
      "Epoch: 2/10 Iteration: 1700 Train loss: 0.093\n",
      "Val acc: 0.857\n",
      "Epoch: 2/10 Iteration: 1750 Train loss: 0.084\n",
      "Epoch: 2/10 Iteration: 1800 Train loss: 0.089\n",
      "Val acc: 0.854\n",
      "Epoch: 2/10 Iteration: 1850 Train loss: 0.075\n",
      "Epoch: 2/10 Iteration: 1900 Train loss: 0.086\n",
      "Val acc: 0.856\n",
      "Epoch: 2/10 Iteration: 1950 Train loss: 0.083\n",
      "Epoch: 2/10 Iteration: 2000 Train loss: 0.083\n",
      "Val acc: 0.856\n",
      "Epoch: 2/10 Iteration: 2050 Train loss: 0.090\n",
      "Epoch: 2/10 Iteration: 2100 Train loss: 0.098\n",
      "Val acc: 0.855\n",
      "Epoch: 2/10 Iteration: 2150 Train loss: 0.067\n",
      "Epoch: 2/10 Iteration: 2200 Train loss: 0.089\n",
      "Val acc: 0.856\n",
      "Epoch: 2/10 Iteration: 2250 Train loss: 0.079\n",
      "Epoch: 2/10 Iteration: 2300 Train loss: 0.102\n",
      "Val acc: 0.855\n",
      "Epoch: 2/10 Iteration: 2350 Train loss: 0.088\n",
      "Epoch: 2/10 Iteration: 2400 Train loss: 0.081\n",
      "Val acc: 0.854\n",
      "Epoch: 3/10 Iteration: 2450 Train loss: 0.083\n",
      "Epoch: 3/10 Iteration: 2500 Train loss: 0.081\n",
      "Val acc: 0.856\n",
      "Epoch: 3/10 Iteration: 2550 Train loss: 0.075\n",
      "Epoch: 3/10 Iteration: 2600 Train loss: 0.079\n",
      "Val acc: 0.854\n",
      "Epoch: 3/10 Iteration: 2650 Train loss: 0.066\n",
      "Epoch: 3/10 Iteration: 2700 Train loss: 0.077\n",
      "Val acc: 0.855\n",
      "Epoch: 3/10 Iteration: 2750 Train loss: 0.079\n",
      "Epoch: 3/10 Iteration: 2800 Train loss: 0.077\n",
      "Val acc: 0.854\n",
      "Epoch: 3/10 Iteration: 2850 Train loss: 0.084\n",
      "Epoch: 3/10 Iteration: 2900 Train loss: 0.090\n",
      "Val acc: 0.852\n",
      "Epoch: 3/10 Iteration: 2950 Train loss: 0.057\n",
      "Epoch: 3/10 Iteration: 3000 Train loss: 0.082\n",
      "Val acc: 0.854\n",
      "Epoch: 3/10 Iteration: 3050 Train loss: 0.067\n",
      "Epoch: 3/10 Iteration: 3100 Train loss: 0.094\n",
      "Val acc: 0.855\n",
      "Epoch: 3/10 Iteration: 3150 Train loss: 0.080\n",
      "Epoch: 3/10 Iteration: 3200 Train loss: 0.079\n",
      "Val acc: 0.854\n",
      "Epoch: 4/10 Iteration: 3250 Train loss: 0.074\n",
      "Epoch: 4/10 Iteration: 3300 Train loss: 0.071\n",
      "Val acc: 0.854\n",
      "Epoch: 4/10 Iteration: 3350 Train loss: 0.059\n",
      "Epoch: 4/10 Iteration: 3400 Train loss: 0.074\n",
      "Val acc: 0.854\n",
      "Epoch: 4/10 Iteration: 3450 Train loss: 0.059\n",
      "Epoch: 4/10 Iteration: 3500 Train loss: 0.075\n",
      "Val acc: 0.854\n",
      "Epoch: 4/10 Iteration: 3550 Train loss: 0.074\n",
      "Epoch: 4/10 Iteration: 3600 Train loss: 0.071\n",
      "Val acc: 0.851\n",
      "Epoch: 4/10 Iteration: 3650 Train loss: 0.077\n",
      "Epoch: 4/10 Iteration: 3700 Train loss: 0.080\n",
      "Val acc: 0.852\n",
      "Epoch: 4/10 Iteration: 3750 Train loss: 0.053\n",
      "Epoch: 4/10 Iteration: 3800 Train loss: 0.071\n",
      "Val acc: 0.852\n",
      "Epoch: 4/10 Iteration: 3850 Train loss: 0.062\n",
      "Epoch: 4/10 Iteration: 3900 Train loss: 0.092\n",
      "Val acc: 0.852\n",
      "Epoch: 4/10 Iteration: 3950 Train loss: 0.073\n",
      "Epoch: 4/10 Iteration: 4000 Train loss: 0.068\n",
      "Val acc: 0.854\n",
      "Epoch: 5/10 Iteration: 4050 Train loss: 0.074\n",
      "Epoch: 5/10 Iteration: 4100 Train loss: 0.071\n",
      "Val acc: 0.852\n",
      "Epoch: 5/10 Iteration: 4150 Train loss: 0.054\n",
      "Epoch: 5/10 Iteration: 4200 Train loss: 0.072\n",
      "Val acc: 0.851\n",
      "Epoch: 5/10 Iteration: 4250 Train loss: 0.049\n",
      "Epoch: 5/10 Iteration: 4300 Train loss: 0.075\n",
      "Val acc: 0.855\n",
      "Epoch: 5/10 Iteration: 4350 Train loss: 0.073\n",
      "Epoch: 5/10 Iteration: 4400 Train loss: 0.058\n",
      "Val acc: 0.848\n",
      "Epoch: 5/10 Iteration: 4450 Train loss: 0.069\n",
      "Epoch: 5/10 Iteration: 4500 Train loss: 0.070\n",
      "Val acc: 0.851\n",
      "Epoch: 5/10 Iteration: 4550 Train loss: 0.041\n",
      "Epoch: 5/10 Iteration: 4600 Train loss: 0.068\n",
      "Val acc: 0.853\n",
      "Epoch: 5/10 Iteration: 4650 Train loss: 0.053\n",
      "Epoch: 5/10 Iteration: 4700 Train loss: 0.080\n",
      "Val acc: 0.851\n",
      "Epoch: 5/10 Iteration: 4750 Train loss: 0.069\n",
      "Epoch: 5/10 Iteration: 4800 Train loss: 0.064\n",
      "Val acc: 0.852\n",
      "Epoch: 6/10 Iteration: 4850 Train loss: 0.060\n",
      "Epoch: 6/10 Iteration: 4900 Train loss: 0.060\n",
      "Val acc: 0.852\n",
      "Epoch: 6/10 Iteration: 4950 Train loss: 0.051\n",
      "Epoch: 6/10 Iteration: 5000 Train loss: 0.060\n",
      "Val acc: 0.849\n",
      "Epoch: 6/10 Iteration: 5050 Train loss: 0.051\n",
      "Epoch: 6/10 Iteration: 5100 Train loss: 0.062\n",
      "Val acc: 0.853\n",
      "Epoch: 6/10 Iteration: 5150 Train loss: 0.064\n",
      "Epoch: 6/10 Iteration: 5200 Train loss: 0.063\n",
      "Val acc: 0.849\n",
      "Epoch: 6/10 Iteration: 5250 Train loss: 0.062\n",
      "Epoch: 6/10 Iteration: 5300 Train loss: 0.066\n",
      "Val acc: 0.850\n",
      "Epoch: 6/10 Iteration: 5350 Train loss: 0.040\n",
      "Epoch: 6/10 Iteration: 5400 Train loss: 0.056\n",
      "Val acc: 0.852\n",
      "Epoch: 6/10 Iteration: 5450 Train loss: 0.054\n",
      "Epoch: 6/10 Iteration: 5500 Train loss: 0.079\n",
      "Val acc: 0.853\n",
      "Epoch: 6/10 Iteration: 5550 Train loss: 0.065\n",
      "Epoch: 6/10 Iteration: 5600 Train loss: 0.061\n",
      "Val acc: 0.850\n",
      "Epoch: 7/10 Iteration: 5650 Train loss: 0.060\n",
      "Epoch: 7/10 Iteration: 5700 Train loss: 0.056\n",
      "Val acc: 0.848\n",
      "Epoch: 7/10 Iteration: 5750 Train loss: 0.049\n",
      "Epoch: 7/10 Iteration: 5800 Train loss: 0.050\n",
      "Val acc: 0.847\n",
      "Epoch: 7/10 Iteration: 5850 Train loss: 0.050\n",
      "Epoch: 7/10 Iteration: 5900 Train loss: 0.065\n",
      "Val acc: 0.852\n",
      "Epoch: 7/10 Iteration: 5950 Train loss: 0.053\n",
      "Epoch: 7/10 Iteration: 6000 Train loss: 0.059\n",
      "Val acc: 0.847\n",
      "Epoch: 7/10 Iteration: 6050 Train loss: 0.056\n",
      "Epoch: 7/10 Iteration: 6100 Train loss: 0.062\n",
      "Val acc: 0.850\n",
      "Epoch: 7/10 Iteration: 6150 Train loss: 0.040\n",
      "Epoch: 7/10 Iteration: 6200 Train loss: 0.051\n",
      "Val acc: 0.851\n",
      "Epoch: 7/10 Iteration: 6250 Train loss: 0.057\n",
      "Epoch: 7/10 Iteration: 6300 Train loss: 0.078\n",
      "Val acc: 0.852\n",
      "Epoch: 7/10 Iteration: 6350 Train loss: 0.063\n",
      "Epoch: 7/10 Iteration: 6400 Train loss: 0.056\n",
      "Val acc: 0.848\n",
      "Epoch: 8/10 Iteration: 6450 Train loss: 0.053\n",
      "Epoch: 8/10 Iteration: 6500 Train loss: 0.057\n",
      "Val acc: 0.849\n",
      "Epoch: 8/10 Iteration: 6550 Train loss: 0.048\n",
      "Epoch: 8/10 Iteration: 6600 Train loss: 0.044\n",
      "Val acc: 0.846\n",
      "Epoch: 8/10 Iteration: 6650 Train loss: 0.043\n",
      "Epoch: 8/10 Iteration: 6700 Train loss: 0.062\n",
      "Val acc: 0.851\n",
      "Epoch: 8/10 Iteration: 6750 Train loss: 0.051\n",
      "Epoch: 8/10 Iteration: 6800 Train loss: 0.049\n",
      "Val acc: 0.847\n",
      "Epoch: 8/10 Iteration: 6850 Train loss: 0.063\n",
      "Epoch: 8/10 Iteration: 6900 Train loss: 0.059\n",
      "Val acc: 0.851\n",
      "Epoch: 8/10 Iteration: 6950 Train loss: 0.036\n",
      "Epoch: 8/10 Iteration: 7000 Train loss: 0.050\n",
      "Val acc: 0.851\n",
      "Epoch: 8/10 Iteration: 7050 Train loss: 0.046\n",
      "Epoch: 8/10 Iteration: 7100 Train loss: 0.064\n",
      "Val acc: 0.850\n",
      "Epoch: 8/10 Iteration: 7150 Train loss: 0.057\n",
      "Epoch: 8/10 Iteration: 7200 Train loss: 0.055\n",
      "Val acc: 0.849\n",
      "Epoch: 9/10 Iteration: 7250 Train loss: 0.044\n",
      "Epoch: 9/10 Iteration: 7300 Train loss: 0.048\n",
      "Val acc: 0.849\n",
      "Epoch: 9/10 Iteration: 7350 Train loss: 0.043\n",
      "Epoch: 9/10 Iteration: 7400 Train loss: 0.045\n",
      "Val acc: 0.845\n",
      "Epoch: 9/10 Iteration: 7450 Train loss: 0.033\n",
      "Epoch: 9/10 Iteration: 7500 Train loss: 0.064\n",
      "Val acc: 0.848\n",
      "Epoch: 9/10 Iteration: 7550 Train loss: 0.047\n",
      "Epoch: 9/10 Iteration: 7600 Train loss: 0.045\n",
      "Val acc: 0.849\n",
      "Epoch: 9/10 Iteration: 7650 Train loss: 0.051\n",
      "Epoch: 9/10 Iteration: 7700 Train loss: 0.056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.848\n",
      "Epoch: 9/10 Iteration: 7750 Train loss: 0.040\n",
      "Epoch: 9/10 Iteration: 7800 Train loss: 0.052\n",
      "Val acc: 0.851\n",
      "Epoch: 9/10 Iteration: 7850 Train loss: 0.045\n",
      "Epoch: 9/10 Iteration: 7900 Train loss: 0.051\n",
      "Val acc: 0.851\n",
      "Epoch: 9/10 Iteration: 7950 Train loss: 0.057\n",
      "Epoch: 9/10 Iteration: 8000 Train loss: 0.049\n",
      "Val acc: 0.850\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "#super params\n",
    "epochs=10\n",
    "saver=tf.train.Saver()\n",
    "with tf.Session()as sess:\n",
    "    writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x_train, x_test, y_train, y_test=train_test_split(X, Y, test_size=0.2)#get_data(index_dict,word_vectors,combined,Y)\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        for ii, (x, y) in enumerate(get_batching(x_train, y_train, batch_size), 1):\n",
    "            feed={input_x:x,label:y,lr:0.001,keep_prob:0.5}\n",
    "            loss,_=sess.run([cost,optimizer],feed_dict=feed)\n",
    "            tf.summary.scalar('loss',loss)\n",
    "            if iteration%50==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "            if iteration%100==0:\n",
    "                val_acc = []\n",
    "                for x, y in get_batching(x_test, y_test, batch_size):\n",
    "                    feed = {input_x: x,\n",
    "                            label: y,                            \n",
    "                            keep_prob: 1}\n",
    "                    batch_acc= sess.run(accuracy, feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                tf.summary.scalar('accuracy',np.mean(val_acc))\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data=pd.read_csv('data/predict_first.csv')\n",
    "predict_sententces=[cut(x) for x in predict_data['Discuss']]\n",
    "predict_X=get_sentence2int(predict_sententces,vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/sentiment.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver=tf.train.Saver()\n",
    "model_file=tf.train.latest_checkpoint('checkpoints/')\n",
    "with tf.Session()as sess:\n",
    "    #sess.run(tf.global_variables_initializer()) \n",
    "    saver.restore(sess,model_file)\n",
    "    predict_arr=[]\n",
    "    for ii in range(0,len(predict_X),100):\n",
    "        for p in sess.run(predictions,feed_dict={input_x:predict_X[ii:ii+100],keep_prob:1.0}):\n",
    "            predict_arr.append(np.argmax(p)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}